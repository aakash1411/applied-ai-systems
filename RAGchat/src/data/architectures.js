import {
  FileText, Layers, GitBranch, Lightbulb,
  Search, ShieldCheck, Brain,
} from 'lucide-react'

export const architectures = [
  {
    id: 'basic',
    name: 'Basic RAG',
    icon: FileText,
    color: '#6366f1',
    shortDesc: 'Simple retrieve-and-generate pipeline',
    description: 'The foundational RAG approach. Documents are chunked into fixed-size pieces, embedded into vectors, and stored. At query time, the most similar chunks are retrieved and fed to the LLM as context.',
    steps: [
      { id: 'chunk', label: 'Chunk', desc: 'Split document into fixed-size chunks (512 words, 50 overlap)' },
      { id: 'embed', label: 'Embed', desc: 'Convert chunks to vector embeddings' },
      { id: 'retrieve', label: 'Retrieve', desc: 'Find top-5 most similar chunks via cosine similarity' },
      { id: 'generate', label: 'Generate', desc: 'LLM generates answer from retrieved context' },
    ],
  },
  {
    id: 'sentenceWindow',
    name: 'Sentence Window',
    icon: Layers,
    color: '#8b5cf6',
    shortDesc: 'Fine-grained retrieval with expanded context',
    description: 'Retrieves at the sentence level for precision, then expands to a window of surrounding sentences for context. Balances retrieval accuracy with sufficient context for generation.',
    steps: [
      { id: 'chunk', label: 'Split', desc: 'Split into individual sentences' },
      { id: 'embed', label: 'Embed', desc: 'Embed each sentence' },
      { id: 'retrieve', label: 'Retrieve', desc: 'Find most relevant sentences' },
      { id: 'expand', label: 'Expand', desc: 'Expand to ±2 surrounding sentences' },
      { id: 'generate', label: 'Generate', desc: 'Generate from expanded window context' },
    ],
  },
  {
    id: 'parentChild',
    name: 'Parent-Child',
    icon: GitBranch,
    color: '#ec4899',
    shortDesc: 'Hierarchical chunks for precise retrieval + broad context',
    description: 'Creates two levels of chunks: small child chunks for precise retrieval and large parent chunks for context. Retrieves by child, returns the parent — giving the LLM more complete context.',
    steps: [
      { id: 'chunk', label: 'Chunk', desc: 'Create parent (1024w) and child (256w) chunks' },
      { id: 'embed', label: 'Embed', desc: 'Embed child chunks' },
      { id: 'retrieve', label: 'Retrieve', desc: 'Find matching child chunks' },
      { id: 'fetch-parent', label: 'Get Parent', desc: 'Fetch parent chunks for full context' },
      { id: 'generate', label: 'Generate', desc: 'Generate from parent context' },
    ],
  },
  {
    id: 'hyde',
    name: 'HyDE',
    icon: Lightbulb,
    color: '#f59e0b',
    shortDesc: 'Hypothetical document embeddings for better retrieval',
    description: 'Instead of embedding the raw query, the LLM first generates a hypothetical answer. This hypothetical document is then embedded and used for retrieval — often finding better matches than the original query.',
    steps: [
      { id: 'chunk', label: 'Chunk', desc: 'Standard chunking' },
      { id: 'embed', label: 'Embed', desc: 'Embed document chunks' },
      { id: 'hypothesize', label: 'Hypothesize', desc: 'LLM generates a hypothetical answer' },
      { id: 'retrieve', label: 'Retrieve', desc: 'Retrieve using hypothetical embedding' },
      { id: 'generate', label: 'Generate', desc: 'Generate final answer from real context' },
    ],
  },
  {
    id: 'multiQuery',
    name: 'Multi-Query',
    icon: Search,
    color: '#22c55e',
    shortDesc: 'Multiple query variations for broader retrieval',
    description: 'Generates multiple variations of the user query, retrieves for each variation, then merges results using Reciprocal Rank Fusion. Captures different aspects of the question for more comprehensive retrieval.',
    steps: [
      { id: 'chunk', label: 'Chunk', desc: 'Standard chunking' },
      { id: 'embed', label: 'Embed', desc: 'Embed document chunks' },
      { id: 'expand-queries', label: 'Expand', desc: 'Generate 3 query variations' },
      { id: 'retrieve', label: 'Retrieve', desc: 'Retrieve for each query variant' },
      { id: 'merge', label: 'Merge', desc: 'Reciprocal Rank Fusion to merge results' },
      { id: 'generate', label: 'Generate', desc: 'Generate from merged context' },
    ],
  },
  {
    id: 'crag',
    name: 'Corrective RAG',
    icon: ShieldCheck,
    color: '#ef4444',
    shortDesc: 'Self-correcting retrieval with relevance evaluation',
    description: 'After retrieval, the LLM evaluates each chunk\'s relevance. If most chunks are irrelevant, the query is refined and retrieval is retried. Ensures high-quality context before generation.',
    steps: [
      { id: 'chunk', label: 'Chunk', desc: 'Standard chunking' },
      { id: 'embed', label: 'Embed', desc: 'Embed document chunks' },
      { id: 'retrieve', label: 'Retrieve', desc: 'Retrieve candidate chunks' },
      { id: 'evaluate', label: 'Evaluate', desc: 'LLM scores chunk relevance (1-5)' },
      { id: 'generate', label: 'Generate', desc: 'Generate from filtered context' },
    ],
  },
  {
    id: 'selfRag',
    name: 'Self-RAG',
    icon: Brain,
    color: '#06b6d4',
    shortDesc: 'Self-reflecting generation with quality control',
    description: 'Generates an initial answer, then self-critiques it for accuracy and completeness. If the critique fails, it retrieves additional context and regenerates. Built-in quality assurance loop.',
    steps: [
      { id: 'chunk', label: 'Chunk', desc: 'Standard chunking' },
      { id: 'embed', label: 'Embed', desc: 'Embed document chunks' },
      { id: 'retrieve', label: 'Retrieve', desc: 'Retrieve relevant chunks' },
      { id: 'generate', label: 'Generate', desc: 'Generate initial answer' },
      { id: 'critique', label: 'Critique', desc: 'Self-evaluate answer quality' },
      { id: 'refine', label: 'Refine', desc: 'Re-retrieve and regenerate if needed' },
    ],
  },
]
